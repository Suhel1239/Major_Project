{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7485793,"sourceType":"datasetVersion","datasetId":4344943},{"sourceId":11048576,"sourceType":"datasetVersion","datasetId":6882814}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.display import clear_output\n!pip install vit_keras\n!pip install colorama\nclear_output()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install -r /kaggle/input/requirement/requirement.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T05:19:56.410840Z","iopub.execute_input":"2025-03-18T05:19:56.411675Z","iopub.status.idle":"2025-03-18T05:20:08.000888Z","shell.execute_reply.started":"2025-03-18T05:19:56.411628Z","shell.execute_reply":"2025-03-18T05:20:07.999483Z"},"_kg_hide-output":true,"scrolled":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import CLIPProcessor, CLIPModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T06:36:05.933788Z","iopub.execute_input":"2025-03-18T06:36:05.934596Z","iopub.status.idle":"2025-03-18T06:36:08.209331Z","shell.execute_reply.started":"2025-03-18T06:36:05.934570Z","shell.execute_reply":"2025-03-18T06:36:08.208644Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nclip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T06:36:28.579767Z","iopub.execute_input":"2025-03-18T06:36:28.580804Z","iopub.status.idle":"2025-03-18T06:36:29.731212Z","shell.execute_reply.started":"2025-03-18T06:36:28.580762Z","shell.execute_reply":"2025-03-18T06:36:29.730228Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls ~/.cache/huggingface/hub | grep p-vit-base-p","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T06:36:32.360079Z","iopub.execute_input":"2025-03-18T06:36:32.360644Z","iopub.status.idle":"2025-03-18T06:36:33.414384Z","shell.execute_reply.started":"2025-03-18T06:36:32.360613Z","shell.execute_reply":"2025-03-18T06:36:33.413475Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clip_model.num_parameters()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T05:20:59.882593Z","iopub.execute_input":"2025-03-18T05:20:59.883426Z","iopub.status.idle":"2025-03-18T05:20:59.890262Z","shell.execute_reply.started":"2025-03-18T05:20:59.883397Z","shell.execute_reply":"2025-03-18T05:20:59.889448Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dir(clip_model)\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_text_embedding(text: str):\n    inputs = clip_processor(text=[text], return_tensors=\"pt\", padding=True)\n    text_embeddings = clip_model.get_text_features(**inputs)\n    return text_embeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T06:36:37.251133Z","iopub.execute_input":"2025-03-18T06:36:37.251492Z","iopub.status.idle":"2025-03-18T06:36:37.256964Z","shell.execute_reply.started":"2025-03-18T06:36:37.251461Z","shell.execute_reply":"2025-03-18T06:36:37.256048Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import requests\nfrom PIL import Image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T06:36:39.384389Z","iopub.execute_input":"2025-03-18T06:36:39.384771Z","iopub.status.idle":"2025-03-18T06:36:39.388415Z","shell.execute_reply.started":"2025-03-18T06:36:39.384740Z","shell.execute_reply":"2025-03-18T06:36:39.387550Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to get image embedding\n# def get_image_embedding(image_url: str):\n#     image = Image.open(requests.get(image_url, stream=True, headers={\"User-Agent\": \"AlexCrawler/1.0 (alexgalea.ca; agalea91@gmail.com)\"}).raw)\n#     inputs = clip_processor(images=image, return_tensors=\"pt\")\n#     image_embeddings = clip_model.get_image_features(**inputs)\n#     return image_embeddings\ndef get_image_embedding(image_path: str):\n    image = Image.open(image_path).convert(\"RGB\")  # Ensure RGB mode\n    image = clip_processor(images=image , return_tensors = \"pt\")  # Preprocess & add batch dimension\n    image_embeddings =  clip_model.get_image_features(**image)\n    # image_embeddings /= image_embeddings.norm(dim=-1, keepdim=True)  # Normalize embeddings\n    \n    return image_embeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T06:36:42.727384Z","iopub.execute_input":"2025-03-18T06:36:42.728182Z","iopub.status.idle":"2025-03-18T06:36:42.732777Z","shell.execute_reply.started":"2025-03-18T06:36:42.728152Z","shell.execute_reply":"2025-03-18T06:36:42.731901Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_url = \"/kaggle/input/satellite-image-caption-generation/train/00001.jpg\"\nimage_embedding = get_image_embedding(image_url)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T05:21:16.018290Z","iopub.execute_input":"2025-03-18T05:21:16.018627Z","iopub.status.idle":"2025-03-18T05:21:16.271381Z","shell.execute_reply.started":"2025-03-18T05:21:16.018602Z","shell.execute_reply":"2025-03-18T05:21:16.270369Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_embedding.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T05:21:19.901361Z","iopub.execute_input":"2025-03-18T05:21:19.902137Z","iopub.status.idle":"2025-03-18T05:21:19.908802Z","shell.execute_reply.started":"2025-03-18T05:21:19.902096Z","shell.execute_reply":"2025-03-18T05:21:19.907964Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import Image as DisplayImage\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T05:21:21.617834Z","iopub.execute_input":"2025-03-18T05:21:21.618163Z","iopub.status.idle":"2025-03-18T05:21:21.622576Z","shell.execute_reply.started":"2025-03-18T05:21:21.618140Z","shell.execute_reply":"2025-03-18T05:21:21.621501Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DisplayImage(image_url)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T05:21:23.948405Z","iopub.execute_input":"2025-03-18T05:21:23.948765Z","iopub.status.idle":"2025-03-18T05:21:23.955905Z","shell.execute_reply.started":"2025-03-18T05:21:23.948739Z","shell.execute_reply":"2025-03-18T05:21:23.955088Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text = \"there are many buildings\"\ntext_embedding = get_text_embedding(text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T05:21:27.115063Z","iopub.execute_input":"2025-03-18T05:21:27.115944Z","iopub.status.idle":"2025-03-18T05:21:27.162873Z","shell.execute_reply.started":"2025-03-18T05:21:27.115914Z","shell.execute_reply":"2025-03-18T05:21:27.162077Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T06:36:48.268411Z","iopub.execute_input":"2025-03-18T06:36:48.269119Z","iopub.status.idle":"2025-03-18T06:36:48.272818Z","shell.execute_reply.started":"2025-03-18T06:36:48.269090Z","shell.execute_reply":"2025-03-18T06:36:48.271972Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.nn.CosineSimilarity()(text_embedding, image_embedding)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T05:21:32.150382Z","iopub.execute_input":"2025-03-18T05:21:32.151205Z","iopub.status.idle":"2025-03-18T05:21:32.176182Z","shell.execute_reply.started":"2025-03-18T05:21:32.151172Z","shell.execute_reply":"2025-03-18T05:21:32.175489Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T06:36:51.260211Z","iopub.execute_input":"2025-03-18T06:36:51.261012Z","iopub.status.idle":"2025-03-18T06:36:51.264484Z","shell.execute_reply.started":"2025-03-18T06:36:51.260982Z","shell.execute_reply":"2025-03-18T06:36:51.263664Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_path = '/kaggle/input/satellite-image-caption-generation/'\n\ntrain_data = pd.read_csv(\"/kaggle/input/satellite-image-caption-generation/train.csv\")\ntrain_data['filepath'] = image_path + train_data['filepath']\n\nvalid_data = pd.read_csv(\"/kaggle/input/satellite-image-caption-generation/valid.csv\")\nvalid_data['filepath'] = image_path + valid_data['filepath']\n\ntest_data = pd.read_csv(\"/kaggle/input/satellite-image-caption-generation/test.csv\")\ntest_data['filepath'] = image_path + test_data['filepath']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T06:36:53.209581Z","iopub.execute_input":"2025-03-18T06:36:53.210426Z","iopub.status.idle":"2025-03-18T06:36:53.322121Z","shell.execute_reply.started":"2025-03-18T06:36:53.210395Z","shell.execute_reply":"2025-03-18T06:36:53.321417Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(test_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T06:36:55.535333Z","iopub.execute_input":"2025-03-18T06:36:55.536157Z","iopub.status.idle":"2025-03-18T06:36:55.542004Z","shell.execute_reply.started":"2025-03-18T06:36:55.536127Z","shell.execute_reply":"2025-03-18T06:36:55.541219Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# the first stage \ndef text_preprocessing(data):\n    data['captions'] = data['captions'].apply(lambda x: x.replace(\"[\",\" \"))\n    data['captions'] = data['captions'].apply(lambda x: x.replace(\"]\",\" \"))\n    data['captions'] = data['captions'].apply(lambda x: x.replace(\"''\",\" \"))\n    data['captions'] = data['captions'].apply(lambda x: x.lower())\n    data['captions'] = data['captions'].apply(lambda x: x.replace(\"[^A-Za-z]\",\" \"))\n    data['captions'] = data['captions'].apply(lambda x: x.replace(\"\\s+\",\" \"))\n    data['captions'] = data['captions'].apply(lambda x: \" \".join([word for word in x.split() if len(word)>1]))\n\n    return data\n\n# splitting each caption (due to one image has many captions)\ndef splitting_captions(df):\n    captions_arr = []\n    filepaths_arr = []\n\n    for i in range(df.shape[0]):\n        img = df['filepath'].values[i]\n        captions = re.split(r\"' '\", df['captions'].values[i])\n        for caption in captions:\n            captions_arr.append(caption)\n            filepaths_arr.append(img)\n\n    data = pd.DataFrame({'captions': captions_arr, 'filepath': filepaths_arr})\n\n    return data\n\n# the last stage and\ndef last_preprocessing(data):\n    data['captions'] = data['captions'].apply(lambda x: x.replace(\"'\",\"\"))\n    data['captions'] = data['captions'].apply(lambda x: x.replace(\".\",\"\"))\n    data['captions'] = \"startseq \"+data['captions']+\" endseq\"\n\n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T06:36:58.475999Z","iopub.execute_input":"2025-03-18T06:36:58.476856Z","iopub.status.idle":"2025-03-18T06:36:58.486586Z","shell.execute_reply.started":"2025-03-18T06:36:58.476827Z","shell.execute_reply":"2025-03-18T06:36:58.485617Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T06:37:01.904789Z","iopub.execute_input":"2025-03-18T06:37:01.905122Z","iopub.status.idle":"2025-03-18T06:37:01.909113Z","shell.execute_reply.started":"2025-03-18T06:37:01.905099Z","shell.execute_reply":"2025-03-18T06:37:01.908264Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Applying text preprocessing functions\n\ntrain_data_preprocessed = text_preprocessing(train_data.iloc[:5000,:])\ntrain_data_preprocessed = splitting_captions(train_data_preprocessed)\ntrain_data_new = last_preprocessing(train_data_preprocessed)\n\nvalid_data_preprocessed = text_preprocessing(valid_data)\nvalid_data_preprocessed = splitting_captions(valid_data_preprocessed)\nvalid_data_new = last_preprocessing(valid_data_preprocessed)\n\ntest_data_preprocessed = text_preprocessing(test_data)\ntest_data_preprocessed = splitting_captions(test_data_preprocessed)\ntest_data_new = last_preprocessing(test_data_preprocessed)\n\n\nprint('train shape -> ', train_data_new.shape[0])\nprint('valid shape -> ', valid_data_new.shape[0])\nprint('test shape -> ', test_data_new.shape[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T06:37:03.339193Z","iopub.execute_input":"2025-03-18T06:37:03.339887Z","iopub.status.idle":"2025-03-18T06:37:03.517743Z","shell.execute_reply.started":"2025-03-18T06:37:03.339856Z","shell.execute_reply":"2025-03-18T06:37:03.516891Z"},"scrolled":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data_preprocessed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T06:37:13.607434Z","iopub.execute_input":"2025-03-18T06:37:13.608201Z","iopub.status.idle":"2025-03-18T06:37:13.625542Z","shell.execute_reply.started":"2025-03-18T06:37:13.608160Z","shell.execute_reply":"2025-03-18T06:37:13.624506Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"type(train_data_preprocessed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T06:37:19.130242Z","iopub.execute_input":"2025-03-18T06:37:19.130964Z","iopub.status.idle":"2025-03-18T06:37:19.136128Z","shell.execute_reply.started":"2025-03-18T06:37:19.130933Z","shell.execute_reply":"2025-03-18T06:37:19.135242Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"IMG_SIZE = 224\nBATCH_SIZE = 64\nSEED = 10\nimport tensorflow as tf\nAUTO = tf.data.AUTOTUNE","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T06:37:21.173273Z","iopub.execute_input":"2025-03-18T06:37:21.173610Z","iopub.status.idle":"2025-03-18T06:37:21.178137Z","shell.execute_reply.started":"2025-03-18T06:37:21.173584Z","shell.execute_reply":"2025-03-18T06:37:21.177176Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# an example of training caption\ntrain_data_new.iloc[SEED].captions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T06:37:23.417592Z","iopub.execute_input":"2025-03-18T06:37:23.418044Z","iopub.status.idle":"2025-03-18T06:37:23.424443Z","shell.execute_reply.started":"2025-03-18T06:37:23.418001Z","shell.execute_reply":"2025-03-18T06:37:23.423483Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\nfrom datetime import timedelta\nfrom torch.optim import AdamW","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T06:37:25.638447Z","iopub.execute_input":"2025-03-18T06:37:25.639229Z","iopub.status.idle":"2025-03-18T06:37:25.645840Z","shell.execute_reply.started":"2025-03-18T06:37:25.639188Z","shell.execute_reply":"2025-03-18T06:37:25.644883Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm \nimport itertools","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T06:37:27.851209Z","iopub.execute_input":"2025-03-18T06:37:27.851530Z","iopub.status.idle":"2025-03-18T06:37:27.855438Z","shell.execute_reply.started":"2025-03-18T06:37:27.851499Z","shell.execute_reply":"2025-03-18T06:37:27.854604Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fine-tune the model\n\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n\noptimizer = AdamW(model.parameters(), lr=5e-6)\n\nnum_epochs = 5\nbatch_size = 64\n\nmodel.train()\nmodel.to(0)\nepoch_losses = []\nfor epoch in list(range(num_epochs)):\n    start_time = time.time()\n    epoch_loss = 0\n    for i in tqdm(range(0, len(train_data_preprocessed), batch_size)):\n        batch = train_data_preprocessed.iloc[i:i+batch_size]  # Batch using iloc\n        captions = batch[\"captions\"].tolist()  # Extract captions\n        images = [Image.open(filepath) for filepath in batch[\"filepath\"]]\n        inputs = processor(text=captions, images=images, return_tensors=\"pt\", padding=True , truncation=True)\n        inputs = {k: v.to(0) for k, v in inputs.items()}\n        outputs = model(**inputs)\n\n        # Compute cosine similarity\n        logits_per_image = outputs.logits_per_image\n        logits_per_text = outputs.logits_per_text\n\n        labels = torch.arange(len(images), device=0)\n        image_loss = torch.nn.functional.cross_entropy(logits_per_image, labels)\n        text_loss = torch.nn.functional.cross_entropy(logits_per_text, labels)\n        loss = (image_loss + text_loss) / 2\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        epoch_loss += loss.item()\n\n    \n    avg_epoch_loss = epoch_loss / len(train_data_preprocessed)\n    end_time = time.time()\n    epoch_duration = end_time - start_time\n    epoch_losses.append(avg_epoch_loss)\n    \n    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_epoch_loss:.4f} - Duration: {str(timedelta(seconds=epoch_duration))} seconds\")\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T06:37:35.434335Z","iopub.execute_input":"2025-03-18T06:37:35.434905Z","iopub.status.idle":"2025-03-18T07:03:13.577340Z","shell.execute_reply.started":"2025-03-18T06:37:35.434874Z","shell.execute_reply":"2025-03-18T07:03:13.576477Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T07:08:04.640710Z","iopub.execute_input":"2025-03-18T07:08:04.641072Z","iopub.status.idle":"2025-03-18T07:08:04.645366Z","shell.execute_reply.started":"2025-03-18T07:08:04.641047Z","shell.execute_reply":"2025-03-18T07:08:04.644527Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(epoch_losses)\nplt.xlabel(\"epoch\")\nplt.ylabel(\"average loss\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T07:08:06.549302Z","iopub.execute_input":"2025-03-18T07:08:06.549880Z","iopub.status.idle":"2025-03-18T07:08:06.808737Z","shell.execute_reply.started":"2025-03-18T07:08:06.549852Z","shell.execute_reply":"2025-03-18T07:08:06.807988Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(\"/kaggle/working/floodnet-finetuned-clip-v1-model\")\nprocessor.save_pretrained(\"/kaggle/working/floodnet-finetuned-clip-v1-processor\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T07:08:21.184436Z","iopub.execute_input":"2025-03-18T07:08:21.185268Z","iopub.status.idle":"2025-03-18T07:08:22.120029Z","shell.execute_reply.started":"2025-03-18T07:08:21.185240Z","shell.execute_reply":"2025-03-18T07:08:22.119306Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(\"/Users/suhelkhan/Major_Project/CLIP_model/fined_tuned_CLIP_model\")\nprocessor.save_pretrained(\"/Users/suhelkhan/Major_Project/CLIP_model/fined_tuned_processor\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T07:09:44.278582Z","iopub.execute_input":"2025-03-18T07:09:44.279000Z","iopub.status.idle":"2025-03-18T07:09:45.271713Z","shell.execute_reply.started":"2025-03-18T07:09:44.278971Z","shell.execute_reply":"2025-03-18T07:09:45.270748Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Evaluate prior to fine-tuning\n\n# # Function to evaluate the model\n# def evaluate(model, processor, data, batch_size=32):\n#     labels = [\"A satellite image of a flooded area of land.\", \"A satellite image of a non-flooded area of land.\"]\n#     model.eval()\n#     print(f\"Using {device} device\")\n#     model.to(device)\n#     correct = 0\n#     total = 0\n#     for batch in tqdm(list(batched(data, batch_size))):\n#         captions = np.array([record.caption for record in batch])\n#         images = [Image.open(record.image) for record in batch]\n#         inputs = processor(text=labels, images=images, return_tensors=\"pt\", padding=True)\n#         inputs = {k: v.to(device) for k, v in inputs.items()}\n#         with torch.no_grad():\n#             outputs = model(**inputs)\n#             logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n#             # probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n\n#             pred_idx = torch.argmax(logits_per_image, dim=1)\n#             pred_labels = np.array([labels[pred] for pred in pred_idx])\n#             correct += (pred_labels == captions).sum().item()\n#             total += len(batch)\n\n#     return correct / total","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# accuracy_after_training = evaluate(model, processor, train_data_preprocessed)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"idx = np.where(test_data_preprocessed['captions'] == \"startseq the airport is very large endseq\")[0]\nfor image_idx in np.random.choice(idx, 1):\n    display(Image.open(test_data_preprocessed['filepath'][image_idx]).resize((650,500)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T07:10:45.979655Z","iopub.execute_input":"2025-03-18T07:10:45.980330Z","iopub.status.idle":"2025-03-18T07:10:46.134331Z","shell.execute_reply.started":"2025-03-18T07:10:45.980304Z","shell.execute_reply":"2025-03-18T07:10:46.133488Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision.utils import make_grid\nfrom torchvision.io import read_image \nfrom torchvision import transforms","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T07:10:51.011115Z","iopub.execute_input":"2025-03-18T07:10:51.011433Z","iopub.status.idle":"2025-03-18T07:10:51.260995Z","shell.execute_reply.started":"2025-03-18T07:10:51.011407Z","shell.execute_reply":"2025-03-18T07:10:51.260295Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_transform_images(image_paths):\n    target_size = (240, 240)\n    reshape_transform = lambda image_tensor: transforms.Resize(target_size)(image_tensor).expand(3, -1, -1)\n    images = [reshape_transform(read_image(image)) for image in image_paths]\n    return images","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T07:10:53.297185Z","iopub.execute_input":"2025-03-18T07:10:53.297943Z","iopub.status.idle":"2025-03-18T07:10:53.302532Z","shell.execute_reply.started":"2025-03-18T07:10:53.297910Z","shell.execute_reply":"2025-03-18T07:10:53.301590Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_image_embeddings(image_paths):\n    # Load the CLIP model\n    clip_model = CLIPModel.from_pretrained(\"/kaggle/working/floodnet-finetuned-clip-v1-model\")\n    clip_processor = CLIPProcessor.from_pretrained(\"/kaggle/working/floodnet-finetuned-clip-v1-processor\")\n\n    images = load_transform_images(image_paths)\n    inputs = clip_processor(images=images, return_tensors=\"pt\")\n    image_embeddings = clip_model.get_image_features(**inputs)\n    return image_embeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T07:10:58.419162Z","iopub.execute_input":"2025-03-18T07:10:58.419599Z","iopub.status.idle":"2025-03-18T07:10:58.424205Z","shell.execute_reply.started":"2025-03-18T07:10:58.419571Z","shell.execute_reply":"2025-03-18T07:10:58.423185Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\nimport torch\nimport numpy as np\nfrom itertools import islice\nfrom tqdm import tqdm\n\n# Function to batch a list\ndef batched(iterable, batch_size):\n    it = iter(iterable)\n    while batch := list(islice(it, batch_size)):\n        yield batch\n\n# Extract image file paths from the DataFrame\nimages = test_data_preprocessed[\"filepath\"].tolist()  # Assuming df is your DataFrame\n\n# Define batch size\nbatch_size = 500\ntotal_batches = int(np.ceil(len(images) / batch_size))\n\ni = 0\nembedding_files = []\nfor image_fps in tqdm(batched(images, batch_size), total=total_batches, desc=\"Processing Batches\"):\n    i += 1\n    print(f\"Processing batch {i}/{total_batches}\")\n\n    # Define output file path\n    file_name = f\"/kaggle/working/images_embeddings_batch_{i}.pt\"\n\n    # Process images and save embeddings\n    torch.save(get_image_embeddings(image_fps), file_name)\n    embedding_files.append(file_name)\n\nprint(\"Embedding processing complete. Saved files:\", embedding_files)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T07:11:06.787056Z","iopub.execute_input":"2025-03-18T07:11:06.787385Z","iopub.status.idle":"2025-03-18T07:17:58.645174Z","shell.execute_reply.started":"2025-03-18T07:11:06.787357Z","shell.execute_reply":"2025-03-18T07:17:58.644301Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embedding_files\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T07:32:32.657419Z","iopub.execute_input":"2025-03-18T07:32:32.658309Z","iopub.status.idle":"2025-03-18T07:32:32.663862Z","shell.execute_reply.started":"2025-03-18T07:32:32.658278Z","shell.execute_reply":"2025-03-18T07:32:32.662903Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_embeddings = torch.cat([torch.load(fp) for fp in embedding_files])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T07:48:32.296092Z","iopub.execute_input":"2025-03-18T07:48:32.296734Z","iopub.status.idle":"2025-03-18T07:48:32.309191Z","shell.execute_reply.started":"2025-03-18T07:48:32.296677Z","shell.execute_reply":"2025-03-18T07:48:32.308440Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_embeddings.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T07:49:11.176073Z","iopub.execute_input":"2025-03-18T07:49:11.176746Z","iopub.status.idle":"2025-03-18T07:49:11.181970Z","shell.execute_reply.started":"2025-03-18T07:49:11.176716Z","shell.execute_reply":"2025-03-18T07:49:11.181076Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sum(image_embedding.element_size() for image_embedding in image_embeddings) / 1e3 # kb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T07:51:02.190939Z","iopub.execute_input":"2025-03-18T07:51:02.191269Z","iopub.status.idle":"2025-03-18T07:51:02.205962Z","shell.execute_reply.started":"2025-03-18T07:51:02.191243Z","shell.execute_reply":"2025-03-18T07:51:02.205051Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def rank_items(text_embedding: torch.Tensor, image_embeddings: torch.Tensor):\n    scores = []\n    cosine_similarity = torch.nn.CosineSimilarity()\n    for image_embedding in tqdm(image_embeddings):\n        score = cosine_similarity(text_embedding.unsqueeze(dim=0), image_embedding.unsqueeze(dim=0))\n        scores.append(float(score.mean().item()))\n    return scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T07:53:34.574237Z","iopub.execute_input":"2025-03-18T07:53:34.574910Z","iopub.status.idle":"2025-03-18T07:53:34.579654Z","shell.execute_reply.started":"2025-03-18T07:53:34.574880Z","shell.execute_reply":"2025-03-18T07:53:34.578766Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scores = rank_items(get_text_embedding(\"startseq there are red buildings and trees endseq\"), image_embeddings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T07:58:04.111043Z","iopub.execute_input":"2025-03-18T07:58:04.111854Z","iopub.status.idle":"2025-03-18T07:58:04.523588Z","shell.execute_reply.started":"2025-03-18T07:58:04.111822Z","shell.execute_reply":"2025-03-18T07:58:04.522756Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T07:58:05.948281Z","iopub.execute_input":"2025-03-18T07:58:05.948609Z","iopub.status.idle":"2025-03-18T07:58:05.959605Z","shell.execute_reply.started":"2025-03-18T07:58:05.948582Z","shell.execute_reply":"2025-03-18T07:58:05.958847Z"},"scrolled":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.argsort(scores)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T07:54:55.000934Z","iopub.execute_input":"2025-03-18T07:54:55.001624Z","iopub.status.idle":"2025-03-18T07:54:55.007712Z","shell.execute_reply.started":"2025-03-18T07:54:55.001595Z","shell.execute_reply":"2025-03-18T07:54:55.006819Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"idx = np.argsort(scores)\nnp.array(scores)[idx][::-1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T07:58:10.926132Z","iopub.execute_input":"2025-03-18T07:58:10.926453Z","iopub.status.idle":"2025-03-18T07:58:10.934019Z","shell.execute_reply.started":"2025-03-18T07:58:10.926428Z","shell.execute_reply":"2025-03-18T07:58:10.933133Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"idx = np.argsort(scores)\nnp.array(test_data_preprocessed['filepath'])[idx][::-1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T07:58:12.563306Z","iopub.execute_input":"2025-03-18T07:58:12.563645Z","iopub.status.idle":"2025-03-18T07:58:12.570188Z","shell.execute_reply.started":"2025-03-18T07:58:12.563617Z","shell.execute_reply":"2025-03-18T07:58:12.569397Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import display\nfrom IPython.display import Image as DisplayImage","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T07:58:14.403345Z","iopub.execute_input":"2025-03-18T07:58:14.403985Z","iopub.status.idle":"2025-03-18T07:58:14.407592Z","shell.execute_reply.started":"2025-03-18T07:58:14.403954Z","shell.execute_reply":"2025-03-18T07:58:14.406675Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"i = 0\nfor image in np.array(images)[idx][::-1]:\n    i += 1\n    display(DisplayImage(image, width=\"200px\"))\n    if i >= 5:\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T07:58:16.177025Z","iopub.execute_input":"2025-03-18T07:58:16.177827Z","iopub.status.idle":"2025-03-18T07:58:16.226294Z","shell.execute_reply.started":"2025-03-18T07:58:16.177795Z","shell.execute_reply":"2025-03-18T07:58:16.225536Z"},"editable":false},"outputs":[],"execution_count":null}]}